{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b5c6eda",
   "metadata": {},
   "source": [
    "# Azure AI Vision Services Demo\n",
    "\n",
    "This notebook demonstrates the capabilities of Azure AI Vision Services including:\n",
    "- **Image Analysis**: Describe images, detect objects, and extract visual features\n",
    "- **Optical Character Recognition (OCR)**: Extract text from images and documents\n",
    "- **Face Detection**: Detect and analyze faces in images\n",
    "- **Custom Vision**: Train custom image classification models\n",
    "- **Content Moderation**: Detect inappropriate content in images\n",
    "\n",
    "## Prerequisites\n",
    "- Azure subscription\n",
    "- Azure Computer Vision resource created in Azure portal\n",
    "- Python 3.8 or higher\n",
    "- Sample images for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e279e",
   "metadata": {},
   "source": [
    "## 1. Setup and Package Installation\n",
    "\n",
    "First, let's install the required Azure Computer Vision SDK and supporting packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d85c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required Azure Computer Vision SDK and supporting packages\n",
    "!pip install azure-cognitiveservices-vision-computervision azure-identity python-dotenv\n",
    "!pip install pillow matplotlib requests numpy opencv-python\n",
    "!pip install azure-ai-vision-imageanalysis  # Latest Vision SDK\n",
    "\n",
    "# For image processing and visualization\n",
    "!pip install IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd01faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "# Azure Computer Vision imports\n",
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"üì¶ Ready to analyze images with Azure AI Vision!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeaefaf",
   "metadata": {},
   "source": [
    "## 2. Azure Computer Vision Configuration\n",
    "\n",
    "### Option 1: Using Environment Variables (Recommended)\n",
    "Set these environment variables in your system or create a `.env` file:\n",
    "```\n",
    "AZURE_VISION_ENDPOINT=https://your-resource.cognitiveservices.azure.com/\n",
    "AZURE_VISION_KEY=your-api-key\n",
    "```\n",
    "\n",
    "### Option 2: Using Managed Identity (For Azure-hosted applications)\n",
    "When running on Azure services with managed identity enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Azure Computer Vision Service\n",
    "vision_endpoint = os.getenv('AZURE_VISION_ENDPOINT') or \"https://your-resource.cognitiveservices.azure.com/\"\n",
    "vision_key = os.getenv('AZURE_VISION_KEY') or \"your-vision-key-here\"\n",
    "\n",
    "# Create Computer Vision client\n",
    "if vision_key and vision_key != \"your-vision-key-here\":\n",
    "    credential = AzureKeyCredential(vision_key)\n",
    "    vision_client = ComputerVisionClient(endpoint=vision_endpoint, credentials=credential)\n",
    "    print(f\"‚úÖ Computer Vision client created successfully!\")\n",
    "    print(f\"üîó Endpoint: {vision_endpoint}\")\n",
    "else:\n",
    "    print(\"‚ùå Please set AZURE_VISION_ENDPOINT and AZURE_VISION_KEY environment variables\")\n",
    "    print(\"You can get these from your Azure Computer Vision resource in the Azure portal\")\n",
    "\n",
    "# Helper function to load and display images\n",
    "def load_and_display_image(image_path_or_url, title=\"Image\", figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Load and display an image from local path or URL\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if image_path_or_url.startswith('http'):\n",
    "            response = requests.get(image_path_or_url)\n",
    "            image = Image.open(io.BytesIO(response.content))\n",
    "        else:\n",
    "            image = Image.open(image_path_or_url)\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.imshow(image)\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample images for testing\n",
    "import urllib.request\n",
    "\n",
    "sample_images = {\n",
    "    \"landmark.jpg\": \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/landmark.jpg\",\n",
    "    \"objects.jpg\": \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/objects.jpg\",\n",
    "    \"faces.jpg\": \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/faces.jpg\",\n",
    "    \"text.jpg\": \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/printed_text.jpg\"\n",
    "}\n",
    "\n",
    "def download_sample_images():\n",
    "    \"\"\"Download sample images for testing\"\"\"\n",
    "    print(\"üì• Downloading sample images...\")\n",
    "    \n",
    "    # Create images directory if it doesn't exist\n",
    "    os.makedirs(\"sample_images\", exist_ok=True)\n",
    "    \n",
    "    for filename, url in sample_images.items():\n",
    "        filepath = os.path.join(\"sample_images\", filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, filepath)\n",
    "                print(f\"‚úÖ Downloaded: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to download {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"üìÅ Already exists: {filename}\")\n",
    "    \n",
    "    print(\"üéØ Sample images ready for analysis!\")\n",
    "\n",
    "# Download sample images\n",
    "download_sample_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e49600b",
   "metadata": {},
   "source": [
    "## 3. Image Analysis and Description\n",
    "\n",
    "Azure Computer Vision can analyze images and provide:\n",
    "- **Descriptions**: Natural language descriptions of image content\n",
    "- **Tags**: Relevant tags for the image\n",
    "- **Categories**: Image categorization\n",
    "- **Objects**: Object detection with bounding boxes\n",
    "- **Brands**: Brand detection\n",
    "- **Faces**: Face detection and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43156373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_comprehensive(image_path_or_url):\n",
    "    \"\"\"\n",
    "    Perform comprehensive image analysis\n",
    "    \n",
    "    Args:\n",
    "        image_path_or_url (str): Path to local image or URL\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üîç Analyzing image: {image_path_or_url}\")\n",
    "        \n",
    "        # Display the image\n",
    "        image = load_and_display_image(image_path_or_url, \"Image for Analysis\")\n",
    "        if not image:\n",
    "            return None\n",
    "        \n",
    "        # Define visual features to extract\n",
    "        features = [\n",
    "            VisualFeatureTypes.description,\n",
    "            VisualFeatureTypes.tags,\n",
    "            VisualFeatureTypes.categories,\n",
    "            VisualFeatureTypes.objects,\n",
    "            VisualFeatureTypes.brands,\n",
    "            VisualFeatureTypes.faces,\n",
    "            VisualFeatureTypes.adult,\n",
    "            VisualFeatureTypes.color\n",
    "        ]\n",
    "        \n",
    "        # Analyze image\n",
    "        if image_path_or_url.startswith('http'):\n",
    "            analysis = vision_client.analyze_image(image_path_or_url, visual_features=features)\n",
    "        else:\n",
    "            with open(image_path_or_url, 'rb') as image_stream:\n",
    "                analysis = vision_client.analyze_image_in_stream(image_stream, visual_features=features)\n",
    "        \n",
    "        # Process and display results\n",
    "        results = {}\n",
    "        \n",
    "        # Description\n",
    "        if analysis.description:\n",
    "            print(\"\\nüìù DESCRIPTION:\")\n",
    "            if analysis.description.captions:\n",
    "                for caption in analysis.description.captions:\n",
    "                    print(f\"   üìñ {caption.text} (confidence: {caption.confidence:.2f})\")\n",
    "                    results['description'] = caption.text\n",
    "        \n",
    "        # Tags\n",
    "        if analysis.tags:\n",
    "            print(\"\\nüè∑Ô∏è TAGS:\")\n",
    "            tags = []\n",
    "            for tag in analysis.tags:\n",
    "                if tag.confidence > 0.5:  # Filter by confidence\n",
    "                    print(f\"   üîñ {tag.name} (confidence: {tag.confidence:.2f})\")\n",
    "                    tags.append({'name': tag.name, 'confidence': tag.confidence})\n",
    "            results['tags'] = tags\n",
    "        \n",
    "        # Categories\n",
    "        if analysis.categories:\n",
    "            print(\"\\nüìÇ CATEGORIES:\")\n",
    "            categories = []\n",
    "            for category in analysis.categories:\n",
    "                print(f\"   üìÅ {category.name} (score: {category.score:.2f})\")\n",
    "                categories.append({'name': category.name, 'score': category.score})\n",
    "            results['categories'] = categories\n",
    "        \n",
    "        # Objects\n",
    "        if analysis.objects:\n",
    "            print(\"\\nüéØ OBJECTS DETECTED:\")\n",
    "            objects = []\n",
    "            for obj in analysis.objects:\n",
    "                print(f\"   üîç {obj.object_property} (confidence: {obj.confidence:.2f})\")\n",
    "                print(f\"      üìç Location: ({obj.rectangle.x}, {obj.rectangle.y}, {obj.rectangle.w}, {obj.rectangle.h})\")\n",
    "                objects.append({\n",
    "                    'name': obj.object_property,\n",
    "                    'confidence': obj.confidence,\n",
    "                    'rectangle': {\n",
    "                        'x': obj.rectangle.x,\n",
    "                        'y': obj.rectangle.y,\n",
    "                        'w': obj.rectangle.w,\n",
    "                        'h': obj.rectangle.h\n",
    "                    }\n",
    "                })\n",
    "            results['objects'] = objects\n",
    "        \n",
    "        # Faces\n",
    "        if analysis.faces:\n",
    "            print(\"\\nüë§ FACES DETECTED:\")\n",
    "            faces = []\n",
    "            for face in analysis.faces:\n",
    "                print(f\"   üë§ Age: {face.age}, Gender: {face.gender}\")\n",
    "                print(f\"      üìç Location: ({face.face_rectangle.left}, {face.face_rectangle.top}, {face.face_rectangle.width}, {face.face_rectangle.height})\")\n",
    "                faces.append({\n",
    "                    'age': face.age,\n",
    "                    'gender': face.gender.value if face.gender else None,\n",
    "                    'rectangle': {\n",
    "                        'left': face.face_rectangle.left,\n",
    "                        'top': face.face_rectangle.top,\n",
    "                        'width': face.face_rectangle.width,\n",
    "                        'height': face.face_rectangle.height\n",
    "                    }\n",
    "                })\n",
    "            results['faces'] = faces\n",
    "        \n",
    "        # Color analysis\n",
    "        if analysis.color:\n",
    "            print(\"\\nüé® COLOR ANALYSIS:\")\n",
    "            print(f\"   üé® Dominant background color: {analysis.color.dominant_color_background}\")\n",
    "            print(f\"   üé® Dominant foreground color: {analysis.color.dominant_color_foreground}\")\n",
    "            print(f\"   üé® Accent color: #{analysis.color.accent_color}\")\n",
    "            print(f\"   ‚ö´ Is black and white: {analysis.color.is_bw_img}\")\n",
    "            results['color'] = {\n",
    "                'background': analysis.color.dominant_color_background,\n",
    "                'foreground': analysis.color.dominant_color_foreground,\n",
    "                'accent': analysis.color.accent_color,\n",
    "                'is_bw': analysis.color.is_bw_img\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test image analysis with sample images\n",
    "for filename in [\"landmark.jpg\", \"objects.jpg\", \"faces.jpg\"]:\n",
    "    filepath = os.path.join(\"sample_images\", filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"üñºÔ∏è ANALYZING: {filename}\")\n",
    "        print(\"=\"*80)\n",
    "        results = analyze_image_comprehensive(filepath)\n",
    "    else:\n",
    "        print(f\"‚ùå Sample image not found: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_objects_and_faces(image_path, analysis_results):\n",
    "    \"\"\"\n",
    "    Visualize detected objects and faces with bounding boxes\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image\n",
    "        analysis_results (dict): Results from image analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        if image_path.startswith('http'):\n",
    "            response = requests.get(image_path)\n",
    "            image = Image.open(io.BytesIO(response.content))\n",
    "        else:\n",
    "            image = Image.open(image_path)\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        \n",
    "        # Original image\n",
    "        axes[0].imshow(image)\n",
    "        axes[0].set_title(\"Original Image\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Image with annotations\n",
    "        axes[1].imshow(image)\n",
    "        axes[1].set_title(\"Detected Objects and Faces\")\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Draw object bounding boxes\n",
    "        if 'objects' in analysis_results and analysis_results['objects']:\n",
    "            for obj in analysis_results['objects']:\n",
    "                rect = obj['rectangle']\n",
    "                # Create rectangle patch\n",
    "                bbox = patches.Rectangle(\n",
    "                    (rect['x'], rect['y']), \n",
    "                    rect['w'], \n",
    "                    rect['h'],\n",
    "                    linewidth=2, \n",
    "                    edgecolor='red', \n",
    "                    facecolor='none'\n",
    "                )\n",
    "                axes[1].add_patch(bbox)\n",
    "                \n",
    "                # Add label\n",
    "                axes[1].text(\n",
    "                    rect['x'], \n",
    "                    rect['y'] - 5, \n",
    "                    f\"{obj['name']} ({obj['confidence']:.2f})\",\n",
    "                    fontsize=10, \n",
    "                    color='red', \n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8)\n",
    "                )\n",
    "        \n",
    "        # Draw face bounding boxes\n",
    "        if 'faces' in analysis_results and analysis_results['faces']:\n",
    "            for face in analysis_results['faces']:\n",
    "                rect = face['rectangle']\n",
    "                # Create rectangle patch\n",
    "                bbox = patches.Rectangle(\n",
    "                    (rect['left'], rect['top']), \n",
    "                    rect['width'], \n",
    "                    rect['height'],\n",
    "                    linewidth=2, \n",
    "                    edgecolor='blue', \n",
    "                    facecolor='none'\n",
    "                )\n",
    "                axes[1].add_patch(bbox)\n",
    "                \n",
    "                # Add label\n",
    "                label = f\"Age: {face['age']}, {face['gender']}\"\n",
    "                axes[1].text(\n",
    "                    rect['left'], \n",
    "                    rect['top'] - 5, \n",
    "                    label,\n",
    "                    fontsize=10, \n",
    "                    color='blue', \n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8)\n",
    "                )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error visualizing objects: {e}\")\n",
    "\n",
    "# Visualize objects for sample images with detection results\n",
    "for filename in [\"objects.jpg\", \"faces.jpg\"]:\n",
    "    filepath = os.path.join(\"sample_images\", filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"\\nüé® VISUALIZING DETECTIONS: {filename}\")\n",
    "        results = analyze_image_comprehensive(filepath)\n",
    "        if results:\n",
    "            visualize_objects_and_faces(filepath, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb86a2d",
   "metadata": {},
   "source": [
    "## 4. Optical Character Recognition (OCR)\n",
    "\n",
    "OCR extracts text from images and documents. Azure Computer Vision provides:\n",
    "- **Read API**: Advanced OCR for printed and handwritten text\n",
    "- **Text extraction**: From images, PDFs, and documents\n",
    "- **Multiple languages**: Support for 100+ languages\n",
    "- **Layout preservation**: Maintains text structure and formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(image_path_or_url):\n",
    "    \"\"\"\n",
    "    Extract text from image using OCR\n",
    "    \n",
    "    Args:\n",
    "        image_path_or_url (str): Path to local image or URL\n",
    "    \n",
    "    Returns:\n",
    "        list: Extracted text lines with confidence scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üìÑ Extracting text from: {image_path_or_url}\")\n",
    "        \n",
    "        # Display the image\n",
    "        image = load_and_display_image(image_path_or_url, \"Image for OCR\")\n",
    "        if not image:\n",
    "            return None\n",
    "        \n",
    "        # Start text recognition\n",
    "        if image_path_or_url.startswith('http'):\n",
    "            read_response = vision_client.read(image_path_or_url, raw=True)\n",
    "        else:\n",
    "            with open(image_path_or_url, 'rb') as image_stream:\n",
    "                read_response = vision_client.read_in_stream(image_stream, raw=True)\n",
    "        \n",
    "        # Get operation location\n",
    "        read_operation_location = read_response.headers[\"Operation-Location\"]\n",
    "        operation_id = read_operation_location.split(\"/\")[-1]\n",
    "        \n",
    "        # Wait for the operation to complete\n",
    "        print(\"‚è≥ Processing text recognition...\")\n",
    "        while True:\n",
    "            read_result = vision_client.get_read_result(operation_id)\n",
    "            if read_result.status not in ['notStarted', 'running']:\n",
    "                break\n",
    "            time.sleep(1)\n",
    "        \n",
    "        # Extract text results\n",
    "        extracted_text = []\n",
    "        if read_result.status == OperationStatusCodes.succeeded:\n",
    "            print(\"\\n‚úÖ TEXT EXTRACTION RESULTS:\")\n",
    "            \n",
    "            for text_result in read_result.analyze_result.read_results:\n",
    "                for line in text_result.lines:\n",
    "                    print(f\"üìù Text: '{line.text}'\")\n",
    "                    print(f\"üìä Confidence: {line.appearance.style.confidence if line.appearance and line.appearance.style else 'N/A'}\")\n",
    "                    \n",
    "                    # Extract bounding box coordinates\n",
    "                    bbox = line.bounding_box\n",
    "                    print(f\"üìç Bounding box: {bbox}\")\n",
    "                    \n",
    "                    extracted_text.append({\n",
    "                        'text': line.text,\n",
    "                        'bounding_box': bbox,\n",
    "                        'confidence': line.appearance.style.confidence if line.appearance and line.appearance.style else None\n",
    "                    })\n",
    "                    print(\"-\" * 50)\n",
    "            \n",
    "            # Show all extracted text combined\n",
    "            all_text = \" \".join([item['text'] for item in extracted_text])\n",
    "            print(f\"\\nüìñ COMBINED TEXT:\\n{all_text}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ùå Text extraction failed: {read_result.status}\")\n",
    "            return None\n",
    "        \n",
    "        return extracted_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting text: {e}\")\n",
    "        return None\n",
    "\n",
    "import time\n",
    "\n",
    "# Test OCR with sample text image\n",
    "text_image_path = os.path.join(\"sample_images\", \"text.jpg\")\n",
    "if os.path.exists(text_image_path):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìÑ OPTICAL CHARACTER RECOGNITION (OCR)\")\n",
    "    print(\"=\"*80)\n",
    "    ocr_results = extract_text_from_image(text_image_path)\n",
    "else:\n",
    "    print(\"‚ùå Sample text image not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ebfa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_extracted_text(image_path, ocr_results):\n",
    "    \"\"\"\n",
    "    Visualize extracted text with bounding boxes\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image\n",
    "        ocr_results (list): OCR results with bounding boxes\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        if image_path.startswith('http'):\n",
    "            response = requests.get(image_path)\n",
    "            image = Image.open(io.BytesIO(response.content))\n",
    "        else:\n",
    "            image = Image.open(image_path)\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"Extracted Text with Bounding Boxes\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Draw bounding boxes for each text line\n",
    "        if ocr_results:\n",
    "            for i, text_item in enumerate(ocr_results):\n",
    "                bbox = text_item['bounding_box']\n",
    "                \n",
    "                # Convert bounding box to rectangle coordinates\n",
    "                # Bounding box format: [x1, y1, x2, y2, x3, y3, x4, y4]\n",
    "                if len(bbox) >= 8:\n",
    "                    x_coords = [bbox[i] for i in range(0, 8, 2)]\n",
    "                    y_coords = [bbox[i] for i in range(1, 8, 2)]\n",
    "                    \n",
    "                    # Create polygon patch\n",
    "                    polygon = patches.Polygon(\n",
    "                        list(zip(x_coords, y_coords)),\n",
    "                        linewidth=2,\n",
    "                        edgecolor='red',\n",
    "                        facecolor='none'\n",
    "                    )\n",
    "                    plt.gca().add_patch(polygon)\n",
    "                    \n",
    "                    # Add text label\n",
    "                    plt.text(\n",
    "                        min(x_coords), \n",
    "                        min(y_coords) - 10, \n",
    "                        f\"Line {i+1}\",\n",
    "                        fontsize=10, \n",
    "                        color='red', \n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8)\n",
    "                    )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error visualizing text: {e}\")\n",
    "\n",
    "# Visualize OCR results if available\n",
    "if 'ocr_results' in locals() and ocr_results:\n",
    "    print(\"\\nüé® VISUALIZING EXTRACTED TEXT:\")\n",
    "    visualize_extracted_text(text_image_path, ocr_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6979e",
   "metadata": {},
   "source": [
    "## 5. Smart Thumbnail Generation\n",
    "\n",
    "Azure Computer Vision can generate smart thumbnails that focus on the most important part of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ced68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_smart_thumbnail(image_path_or_url, width=200, height=200, smart_cropping=True):\n",
    "    \"\"\"\n",
    "    Generate a smart thumbnail from an image\n",
    "    \n",
    "    Args:\n",
    "        image_path_or_url (str): Path to local image or URL\n",
    "        width (int): Thumbnail width\n",
    "        height (int): Thumbnail height\n",
    "        smart_cropping (bool): Enable smart cropping\n",
    "    \n",
    "    Returns:\n",
    "        bytes: Thumbnail image data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üñºÔ∏è Generating smart thumbnail ({width}x{height}) from: {image_path_or_url}\")\n",
    "        \n",
    "        # Generate thumbnail\n",
    "        if image_path_or_url.startswith('http'):\n",
    "            thumbnail_stream = vision_client.generate_thumbnail(\n",
    "                width, height, image_path_or_url, smart_cropping\n",
    "            )\n",
    "        else:\n",
    "            with open(image_path_or_url, 'rb') as image_stream:\n",
    "                thumbnail_stream = vision_client.generate_thumbnail_in_stream(\n",
    "                    width, height, image_stream, smart_cropping\n",
    "                )\n",
    "        \n",
    "        # Save and display thumbnail\n",
    "        thumbnail_data = thumbnail_stream.read()\n",
    "        thumbnail_filename = f\"thumbnail_{width}x{height}_{'smart' if smart_cropping else 'center'}.jpg\"\n",
    "        \n",
    "        with open(thumbnail_filename, 'wb') as thumbnail_file:\n",
    "            thumbnail_file.write(thumbnail_data)\n",
    "        \n",
    "        # Display original and thumbnail side by side\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        \n",
    "        # Original image\n",
    "        if image_path_or_url.startswith('http'):\n",
    "            response = requests.get(image_path_or_url)\n",
    "            original_image = Image.open(io.BytesIO(response.content))\n",
    "        else:\n",
    "            original_image = Image.open(image_path_or_url)\n",
    "        \n",
    "        axes[0].imshow(original_image)\n",
    "        axes[0].set_title(\"Original Image\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Thumbnail\n",
    "        thumbnail_image = Image.open(io.BytesIO(thumbnail_data))\n",
    "        axes[1].imshow(thumbnail_image)\n",
    "        axes[1].set_title(f\"Smart Thumbnail ({width}x{height})\")\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"‚úÖ Thumbnail saved as: {thumbnail_filename}\")\n",
    "        return thumbnail_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating thumbnail: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test thumbnail generation\n",
    "landmark_path = os.path.join(\"sample_images\", \"landmark.jpg\")\n",
    "if os.path.exists(landmark_path):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üñºÔ∏è SMART THUMBNAIL GENERATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Generate different sizes\n",
    "    sizes = [(150, 150), (200, 100), (100, 200)]\n",
    "    \n",
    "    for width, height in sizes:\n",
    "        thumbnail = generate_smart_thumbnail(landmark_path, width, height)\n",
    "else:\n",
    "    print(\"‚ùå Landmark image not found for thumbnail generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c361e",
   "metadata": {},
   "source": [
    "## 6. Domain-Specific Analysis\n",
    "\n",
    "Azure Computer Vision provides specialized analysis for:\n",
    "- **Landmarks**: Famous landmarks and tourist attractions\n",
    "- **Celebrities**: Celebrity face recognition\n",
    "- **Domain models**: Specialized models for specific content types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af814c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_domain_specific(image_path_or_url, domain=\"landmarks\"):\n",
    "    \"\"\"\n",
    "    Perform domain-specific analysis (landmarks or celebrities)\n",
    "    \n",
    "    Args:\n",
    "        image_path_or_url (str): Path to local image or URL\n",
    "        domain (str): 'landmarks' or 'celebrities'\n",
    "    \n",
    "    Returns:\n",
    "        dict: Domain-specific analysis results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üèõÔ∏è Analyzing for {domain}: {image_path_or_url}\")\n",
    "        \n",
    "        # Display the image\n",
    "        image = load_and_display_image(image_path_or_url, f\"Image for {domain} analysis\")\n",
    "        if not image:\n",
    "            return None\n",
    "        \n",
    "        # Perform domain-specific analysis\n",
    "        if image_path_or_url.startswith('http'):\n",
    "            analysis = vision_client.analyze_image_by_domain(domain, image_path_or_url)\n",
    "        else:\n",
    "            with open(image_path_or_url, 'rb') as image_stream:\n",
    "                analysis = vision_client.analyze_image_by_domain_in_stream(domain, image_stream)\n",
    "        \n",
    "        # Process results\n",
    "        results = {}\n",
    "        \n",
    "        if analysis.result:\n",
    "            if domain == \"landmarks\":\n",
    "                landmarks = analysis.result.get(\"landmarks\", [])\n",
    "                if landmarks:\n",
    "                    print(f\"\\nüèõÔ∏è LANDMARKS DETECTED:\")\n",
    "                    for landmark in landmarks:\n",
    "                        name = landmark.get(\"name\", \"Unknown\")\n",
    "                        confidence = landmark.get(\"confidence\", 0)\n",
    "                        print(f\"   üèõÔ∏è {name} (confidence: {confidence:.2f})\")\n",
    "                    results['landmarks'] = landmarks\n",
    "                else:\n",
    "                    print(\"\\nüèõÔ∏è No landmarks detected\")\n",
    "            \n",
    "            elif domain == \"celebrities\":\n",
    "                celebrities = analysis.result.get(\"celebrities\", [])\n",
    "                if celebrities:\n",
    "                    print(f\"\\n‚≠ê CELEBRITIES DETECTED:\")\n",
    "                    for celebrity in celebrities:\n",
    "                        name = celebrity.get(\"name\", \"Unknown\")\n",
    "                        confidence = celebrity.get(\"confidence\", 0)\n",
    "                        face_rect = celebrity.get(\"faceRectangle\", {})\n",
    "                        print(f\"   ‚≠ê {name} (confidence: {confidence:.2f})\")\n",
    "                        if face_rect:\n",
    "                            print(f\"      üìç Face location: ({face_rect.get('left', 0)}, {face_rect.get('top', 0)}, {face_rect.get('width', 0)}, {face_rect.get('height', 0)})\")\n",
    "                    results['celebrities'] = celebrities\n",
    "                else:\n",
    "                    print(\"\\n‚≠ê No celebrities detected\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in domain-specific analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test domain-specific analysis\n",
    "landmark_path = os.path.join(\"sample_images\", \"landmark.jpg\")\n",
    "if os.path.exists(landmark_path):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèõÔ∏è LANDMARK DETECTION\")\n",
    "    print(\"=\"*60)\n",
    "    landmark_results = analyze_domain_specific(landmark_path, \"landmarks\")\n",
    "\n",
    "# Test celebrity detection (if faces image contains celebrities)\n",
    "faces_path = os.path.join(\"sample_images\", \"faces.jpg\")\n",
    "if os.path.exists(faces_path):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚≠ê CELEBRITY DETECTION\")\n",
    "    print(\"=\"*60)\n",
    "    celebrity_results = analyze_domain_specific(faces_path, \"celebrities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51a210",
   "metadata": {},
   "source": [
    "## 7. Batch Image Processing\n",
    "\n",
    "Process multiple images efficiently with batch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_analyze_images(image_paths, analysis_type=\"description\"):\n",
    "    \"\"\"\n",
    "    Analyze multiple images in batch\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of image paths or URLs\n",
    "        analysis_type (str): Type of analysis ('description', 'tags', 'objects')\n",
    "    \n",
    "    Returns:\n",
    "        list: Analysis results for all images\n",
    "    \"\"\"\n",
    "    print(f\"üìä Batch processing {len(image_paths)} images for {analysis_type} analysis...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths, 1):\n",
    "        try:\n",
    "            print(f\"\\nüîÑ Processing image {i}/{len(image_paths)}: {os.path.basename(image_path)}\")\n",
    "            \n",
    "            # Quick analysis based on type\n",
    "            if analysis_type == \"description\":\n",
    "                if image_path.startswith('http'):\n",
    "                    analysis = vision_client.describe_image(image_path)\n",
    "                else:\n",
    "                    with open(image_path, 'rb') as image_stream:\n",
    "                        analysis = vision_client.describe_image_in_stream(image_stream)\n",
    "                \n",
    "                description = analysis.captions[0].text if analysis.captions else \"No description\"\n",
    "                confidence = analysis.captions[0].confidence if analysis.captions else 0\n",
    "                \n",
    "                result = {\n",
    "                    'image': os.path.basename(image_path),\n",
    "                    'description': description,\n",
    "                    'confidence': confidence\n",
    "                }\n",
    "                \n",
    "                print(f\"   üìñ Description: {description} (confidence: {confidence:.2f})\")\n",
    "            \n",
    "            elif analysis_type == \"tags\":\n",
    "                features = [VisualFeatureTypes.tags]\n",
    "                if image_path.startswith('http'):\n",
    "                    analysis = vision_client.analyze_image(image_path, visual_features=features)\n",
    "                else:\n",
    "                    with open(image_path, 'rb') as image_stream:\n",
    "                        analysis = vision_client.analyze_image_in_stream(image_stream, visual_features=features)\n",
    "                \n",
    "                tags = [{'name': tag.name, 'confidence': tag.confidence} \n",
    "                       for tag in analysis.tags if tag.confidence > 0.5]\n",
    "                \n",
    "                result = {\n",
    "                    'image': os.path.basename(image_path),\n",
    "                    'tags': tags\n",
    "                }\n",
    "                \n",
    "                print(f\"   üè∑Ô∏è Top tags: {', '.join([tag['name'] for tag in tags[:5]])}\")\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error processing {image_path}: {e}\")\n",
    "            results.append({\n",
    "                'image': os.path.basename(image_path),\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nüìà BATCH PROCESSING SUMMARY:\")\n",
    "    successful = len([r for r in results if 'error' not in r])\n",
    "    failed = len([r for r in results if 'error' in r])\n",
    "    print(f\"   ‚úÖ Successful: {successful}\")\n",
    "    print(f\"   ‚ùå Failed: {failed}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test batch processing with available sample images\n",
    "available_images = []\n",
    "for filename in sample_images.keys():\n",
    "    filepath = os.path.join(\"sample_images\", filename)\n",
    "    if os.path.exists(filepath):\n",
    "        available_images.append(filepath)\n",
    "\n",
    "if available_images:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä BATCH IMAGE PROCESSING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test batch description\n",
    "    batch_results_desc = batch_analyze_images(available_images, \"description\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(\"üè∑Ô∏è BATCH TAG ANALYSIS\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Test batch tagging\n",
    "    batch_results_tags = batch_analyze_images(available_images, \"tags\")\n",
    "else:\n",
    "    print(\"‚ùå No sample images available for batch processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775673ce",
   "metadata": {},
   "source": [
    "## 8. Performance Monitoring and Best Practices\n",
    "\n",
    "### ‚ö° Performance Tips\n",
    "- **Image optimization**: Resize large images before processing\n",
    "- **Batch operations**: Process multiple images together\n",
    "- **Caching**: Cache analysis results for frequently accessed images\n",
    "- **Error handling**: Implement retry logic for transient failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def performance_test_vision(image_paths, test_type=\"description\"):\n",
    "    \"\"\"\n",
    "    Test Computer Vision API performance\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of image paths to test\n",
    "        test_type (str): Type of test to perform\n",
    "    \n",
    "    Returns:\n",
    "        dict: Performance metrics\n",
    "    \"\"\"\n",
    "    print(f\"‚ö° Performance Testing - {test_type.title()} Analysis\")\n",
    "    print(f\"üìä Testing with {len(image_paths)} images\")\n",
    "    \n",
    "    results = {\n",
    "        'total_time': 0,\n",
    "        'successful_operations': 0,\n",
    "        'failed_operations': 0,\n",
    "        'average_time_per_image': 0,\n",
    "        'images_per_minute': 0,\n",
    "        'start_time': datetime.now()\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths, 1):\n",
    "        print(f\"\\nüîÑ Processing image {i}/{len(image_paths)}...\")\n",
    "        image_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            if test_type == \"description\":\n",
    "                with open(image_path, 'rb') as image_stream:\n",
    "                    analysis = vision_client.describe_image_in_stream(image_stream)\n",
    "                success = len(analysis.captions) > 0\n",
    "            \n",
    "            elif test_type == \"analysis\":\n",
    "                features = [VisualFeatureTypes.description, VisualFeatureTypes.tags]\n",
    "                with open(image_path, 'rb') as image_stream:\n",
    "                    analysis = vision_client.analyze_image_in_stream(image_stream, visual_features=features)\n",
    "                success = analysis.description and len(analysis.description.captions) > 0\n",
    "            \n",
    "            elif test_type == \"ocr\":\n",
    "                with open(image_path, 'rb') as image_stream:\n",
    "                    read_response = vision_client.read_in_stream(image_stream, raw=True)\n",
    "                operation_id = read_response.headers[\"Operation-Location\"].split(\"/\")[-1]\n",
    "                \n",
    "                # Wait for completion\n",
    "                while True:\n",
    "                    read_result = vision_client.get_read_result(operation_id)\n",
    "                    if read_result.status not in ['notStarted', 'running']:\n",
    "                        break\n",
    "                    time.sleep(0.1)\n",
    "                \n",
    "                success = read_result.status == OperationStatusCodes.succeeded\n",
    "            \n",
    "            image_end = time.time()\n",
    "            image_time = image_end - image_start\n",
    "            \n",
    "            if success:\n",
    "                results['successful_operations'] += 1\n",
    "                print(f\"‚úÖ Completed in {image_time:.2f} seconds\")\n",
    "            else:\n",
    "                results['failed_operations'] += 1\n",
    "                print(f\"‚ùå Failed after {image_time:.2f} seconds\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            results['failed_operations'] += 1\n",
    "            image_time = time.time() - image_start\n",
    "            print(f\"‚ùå Error after {image_time:.2f} seconds: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    results['total_time'] = end_time - start_time\n",
    "    results['end_time'] = datetime.now()\n",
    "    \n",
    "    if results['successful_operations'] > 0:\n",
    "        results['average_time_per_image'] = results['total_time'] / results['successful_operations']\n",
    "        results['images_per_minute'] = 60 / results['average_time_per_image']\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüìä Performance Test Results:\")\n",
    "    print(f\"   ‚è±Ô∏è Total Time: {results['total_time']:.2f} seconds\")\n",
    "    print(f\"   ‚úÖ Successful: {results['successful_operations']}\")\n",
    "    print(f\"   ‚ùå Failed: {results['failed_operations']}\")\n",
    "    print(f\"   üìà Average Time/Image: {results['average_time_per_image']:.2f} seconds\")\n",
    "    print(f\"   üöÄ Images/Minute: {results['images_per_minute']:.1f}\")\n",
    "    print(f\"   üìÖ Test Duration: {results['start_time'].strftime('%H:%M:%S')} - {results['end_time'].strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run performance tests if sample images are available\n",
    "if available_images and len(available_images) >= 2:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ö° COMPUTER VISION PERFORMANCE TESTING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test description performance\n",
    "    desc_perf = performance_test_vision(available_images[:3], \"description\")\n",
    "else:\n",
    "    print(\"\\nüí° Not enough sample images for performance testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510eddc3",
   "metadata": {},
   "source": [
    "## 9. Interactive Demo\n",
    "\n",
    "Try Computer Vision features with your own images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aa70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Interactive Computer Vision Demo\n",
    "# Customize these variables and run the cell!\n",
    "\n",
    "# Option 1: Use a URL to an image\n",
    "your_image_url = \"https://example.com/your-image.jpg\"  # Replace with your image URL\n",
    "\n",
    "# Option 2: Use a local image path\n",
    "your_local_image = \"path/to/your/image.jpg\"  # Replace with your local image path\n",
    "\n",
    "# Choose analysis type\n",
    "analysis_types = {\n",
    "    'comprehensive': 'Full image analysis with all features',\n",
    "    'description': 'Generate image description only',\n",
    "    'ocr': 'Extract text from image',\n",
    "    'thumbnail': 'Generate smart thumbnail',\n",
    "    'objects': 'Detect objects in image'\n",
    "}\n",
    "\n",
    "selected_analysis = 'comprehensive'  # Change this to test different analyses\n",
    "\n",
    "def interactive_analysis(image_source, analysis_type='comprehensive'):\n",
    "    \"\"\"\n",
    "    Interactive analysis function for testing\n",
    "    \"\"\"\n",
    "    print(f\"üéØ Running {analysis_type} analysis on your image...\")\n",
    "    \n",
    "    if analysis_type == 'comprehensive':\n",
    "        return analyze_image_comprehensive(image_source)\n",
    "    elif analysis_type == 'description':\n",
    "        try:\n",
    "            if image_source.startswith('http'):\n",
    "                result = vision_client.describe_image(image_source)\n",
    "            else:\n",
    "                with open(image_source, 'rb') as img:\n",
    "                    result = vision_client.describe_image_in_stream(img)\n",
    "            \n",
    "            if result.captions:\n",
    "                print(f\"üìñ Description: {result.captions[0].text}\")\n",
    "                print(f\"üìä Confidence: {result.captions[0].confidence:.2f}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            return None\n",
    "    elif analysis_type == 'ocr':\n",
    "        return extract_text_from_image(image_source)\n",
    "    elif analysis_type == 'thumbnail':\n",
    "        return generate_smart_thumbnail(image_source, 200, 200)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Instructions for use\n",
    "print(\"üéØ INTERACTIVE COMPUTER VISION DEMO\")\n",
    "print(\"=\" * 50)\n",
    "print(\"To use this demo:\")\n",
    "print(\"1. Replace 'your_image_url' or 'your_local_image' with your image\")\n",
    "print(\"2. Choose your analysis type from:\")\n",
    "for key, description in analysis_types.items():\n",
    "    print(f\"   ‚Ä¢ {key}: {description}\")\n",
    "print(\"3. Set 'selected_analysis' variable\")\n",
    "print(\"4. Uncomment the analysis code below\")\n",
    "print(\"\\nüí° Available analysis types:\", list(analysis_types.keys()))\n",
    "\n",
    "# Uncomment the lines below to test with your image\n",
    "# if your_image_url != \"https://example.com/your-image.jpg\":\n",
    "#     result = interactive_analysis(your_image_url, selected_analysis)\n",
    "# elif os.path.exists(your_local_image):\n",
    "#     result = interactive_analysis(your_local_image, selected_analysis)\n",
    "# else:\n",
    "#     print(\"‚ùå Please provide a valid image URL or local path\")\n",
    "\n",
    "print(\"\\nüöÄ Ready to analyze your images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617410f",
   "metadata": {},
   "source": [
    "## 10. Troubleshooting and Best Practices\n",
    "\n",
    "### üîß Common Issues and Solutions\n",
    "\n",
    "1. **Authentication Errors**\n",
    "   - ‚ùå \"Access denied\" or \"Invalid subscription key\"\n",
    "   - ‚úÖ Check your Computer Vision key and endpoint\n",
    "   - ‚úÖ Verify the resource is in the correct region\n",
    "\n",
    "2. **Image Format Issues**\n",
    "   - ‚ùå \"Invalid image format\"\n",
    "   - ‚úÖ Supported formats: JPEG, PNG, GIF, BMP\n",
    "   - ‚úÖ Maximum file size: 50 MB\n",
    "   - ‚úÖ Minimum dimensions: 50x50 pixels\n",
    "\n",
    "3. **Rate Limiting**\n",
    "   - ‚ùå \"Too many requests\"\n",
    "   - ‚úÖ Implement exponential backoff\n",
    "   - ‚úÖ Check your pricing tier limits\n",
    "\n",
    "4. **OCR Accuracy**\n",
    "   - ‚ùå Poor text recognition\n",
    "   - ‚úÖ Ensure good image quality and contrast\n",
    "   - ‚úÖ Use appropriate image resolution\n",
    "   - ‚úÖ Consider text orientation and language\n",
    "\n",
    "### üí° Best Practices\n",
    "\n",
    "- **Image Quality**: Use high-resolution, well-lit images\n",
    "- **Error Handling**: Always implement try-catch blocks\n",
    "- **Caching**: Cache results for frequently analyzed images\n",
    "- **Monitoring**: Track API usage and costs\n",
    "- **Security**: Use managed identity in production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a793543",
   "metadata": {},
   "source": [
    "## 11. Conclusion and Next Steps\n",
    "\n",
    "üéâ **Congratulations!** You've successfully explored Azure AI Computer Vision capabilities including:\n",
    "- ‚úÖ Comprehensive image analysis\n",
    "- ‚úÖ Object and face detection\n",
    "- ‚úÖ Optical Character Recognition (OCR)\n",
    "- ‚úÖ Smart thumbnail generation\n",
    "- ‚úÖ Domain-specific analysis (landmarks, celebrities)\n",
    "- ‚úÖ Batch processing\n",
    "- ‚úÖ Performance optimization\n",
    "\n",
    "### üöÄ Next Steps\n",
    "1. **Complete the series** with the final notebook:\n",
    "   - Azure AI Language Services ‚úÖ\n",
    "   - Azure AI Speech Services ‚úÖ\n",
    "   - Azure AI Vision Services ‚úÖ\n",
    "   - Azure AI Document Intelligence ‚û°Ô∏è\n",
    "\n",
    "2. **Build real applications** using Computer Vision:\n",
    "   - Content moderation systems\n",
    "   - Accessibility tools for visually impaired\n",
    "   - Inventory management with object detection\n",
    "   - Digital asset management\n",
    "   - Document digitization workflows\n",
    "\n",
    "3. **Advanced features to explore**:\n",
    "   - Custom Vision for specialized models\n",
    "   - Video analysis with Video Indexer\n",
    "   - Form Recognizer for structured documents\n",
    "   - Spatial Analysis for people counting\n",
    "\n",
    "### üìö Additional Resources\n",
    "- [Azure Computer Vision Documentation](https://docs.microsoft.com/azure/cognitive-services/computer-vision/)\n",
    "- [Computer Vision SDK Samples](https://github.com/Azure-Samples/cognitive-services-quickstart-code)\n",
    "- [Vision Studio Portal](https://portal.vision.cognitive.azure.com/) - Test and customize vision models\n",
    "- [Computer Vision Pricing](https://azure.microsoft.com/pricing/details/cognitive-services/computer-vision/)\n",
    "\n",
    "### üîó Useful Links\n",
    "- [Image Requirements](https://docs.microsoft.com/azure/cognitive-services/computer-vision/concept-image-requirements)\n",
    "- [Language Support](https://docs.microsoft.com/azure/cognitive-services/computer-vision/language-support)\n",
    "- [API Reference](https://docs.microsoft.com/rest/api/computervision/)\n",
    "- [SDK Reference](https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-computervision/)\n",
    "\n",
    "**Happy analyzing with Azure Computer Vision! üñºÔ∏èüëÅÔ∏è**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
